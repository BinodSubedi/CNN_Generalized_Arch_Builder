{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b36ac7b-2d5c-4569-af14-ab28ce816b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f2fec7-3edc-435b-b765-5023cd49b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9d571c-26fb-4cdd-985d-be243b56323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af9b616-28f1-454f-8e60-5aca3d86fe84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_dataset[0]['image']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4fe0f21-f5ac-49f1-87e1-1392d7f50ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing LeNet\n",
    "# Initial Layer of convolution with feature map/ filter count of 6\n",
    "# Kernel size 5, output size will be (28 - 5) + 1 = 24\n",
    "# Let's build convolution Layer, so that I could generalize any architecture implementation\n",
    "class Conv_Layer:\n",
    "    def __init__(self, input_channel, input_size, stride, filter_shape, output_chanel_or_filter_num):\n",
    "        self.input_channel = input_channel # channel means to multiply with same filter in same place and adding the input_channel times\n",
    "        self.input_size = input_size # size of input or for first conv layer, it will be image resolution\n",
    "        self.stride = stride\n",
    "        self.filter_shape = filter_shape\n",
    "        self.output_chanel = self.output_chanel_or_filter_num # This could also be called the number of filer numbers\n",
    "\n",
    "        # In a normalize standarizarion here,\n",
    "        # the shape of input is (input_channel, input_shape, input_shape)\n",
    "    \n",
    "    def initialize_filer_weights(self):\n",
    "        # This structure defines every filter weights for each input channel, and it goes for each output channel\n",
    "        self.filter_weights = 0.01 * np.random.randn(self.output_chanel, self.input_channel,filter_shape,filter_shape)\n",
    "\n",
    "    def initialize_filter_biases(self):\n",
    "        self.filter_biases = np.zeros(self.output_chanel, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.inputs = input\n",
    "        self.input = input\n",
    "        self.output = []\n",
    "        for i, filter in enumerate(self.filter_weights):\n",
    "            output_per_filter = []\n",
    "            # Now I will iterate through column and through row\n",
    "            for row in range (self.input_size):\n",
    "\n",
    "                output_per_filter_row = []\n",
    "                # This projects from overflowing through columns (Verically)\n",
    "                if(row > self.input_size - self.filter_shape - 1):\n",
    "                    break\n",
    "                    \n",
    "                for column in range (self.input_size):\n",
    "\n",
    "                    # This projects from overflowing through columns (Horizontally)\n",
    "                    if(column > self.input_size - self.filter_shape - 1):\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        # Do multiplication, do slice among input channels, multiply, sum and append to output_per_filter\n",
    "                        multi_channel_filter_feature_block =  self.input[:,row:self.filter_shape + row, column:self.filter_shape + column]\n",
    "                        # Now multiplying each of the arrays with filter, and adding each and each one of them next\n",
    "                        filter_applied_frame = multi_channel_filter_feature_block * filter\n",
    "                        # added all filter per output channel fashion\n",
    "                        output_per_filter_row[column] = filter_applied_frame.sum() + self.filter_biases[i,0]\n",
    "\n",
    "                    column+=self.stride\n",
    "                output_per_filter[row] = output_per_filter_row\n",
    "                row+=self.stride\n",
    "            \n",
    "            self.output[i] = output_per_filter\n",
    "                        \n",
    "\n",
    "    # Need to calculate gradients for both filters and dinputs\n",
    "    # first calculating for dinputs\n",
    "    def backward(self,dvalues):\n",
    "        self.dbiases = np.sum(dvalues, axis=(1,2))\n",
    "        self.dinputs = np.zeros_like(self.inputs)\n",
    "        # this is dweights or dfilter_weights\n",
    "        self.dweights = np.zeros_like(self.filter_weights)\n",
    "        for i, filter in enumerate(self.filter_weights):\n",
    "\n",
    "            # Now I will iterate through column and through row\n",
    "            for row in range (self.output[0].shape[1]):\n",
    "\n",
    "                # This projects from overflowing through columns (Verically)\n",
    "                if(row > self.input_size - self.filter_shape - 1):\n",
    "                    break\n",
    "                    \n",
    "                for column in range (self.output[0].shape[0]):\n",
    "\n",
    "                    # This projects from overflowing through columns (Horizontally)\n",
    "                    if(column > self.input_size - self.filter_shape - 1):\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        # Do multiplication, do slice among input channels, multiply, sum and append to output_per_filter\n",
    "                        self.dinputs[:, row:row + self.filter_shape, column: column + self.filter_shape] += filter * dvalues[i,row,column]\n",
    "                        self.dweights[i, :, :,:] += dvalues[i, row, column] * self.inputs[:, row: row + self.filter_shape, \\\n",
    "                        column: column + self.filter_shape]\n",
    "\n",
    "                    column+=self.stride\n",
    "                \n",
    "                row+=self.stride\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b80706de-f015-4491-975f-799d78ac2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no learning parameters in Pooling\n",
    "class Pooling:\n",
    "    def __init__(self,pooling_frame_size, input_channel, input_size,stride=0):\n",
    "        self.pooling_frame_size = pooling_frame_size\n",
    "        self.input_channel = input_channel\n",
    "        self.input_size = input_size\n",
    "\n",
    "        if(stride != 0):\n",
    "            self.stride = stride\n",
    "        else:\n",
    "            self.stride = self.pooling_frame_size\n",
    "        \n",
    "    def pooling(self,inputs):\n",
    "        pass\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        pass\n",
    "\n",
    "class Average_Pooling(Pooling):\n",
    "    def __init__(self,pooling_frame_size, input_channel, input_size, stride):\n",
    "        super().__init__(pooling_frame_size, input_channel, input_size)\n",
    "        \n",
    "\n",
    "    def pooling(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = []\n",
    "        \n",
    "        for i in range(self.input_channel):\n",
    "            output_per_filter = []\n",
    "            # Now I will iterate through column and through row\n",
    "            for j in range (self.input_size):\n",
    "\n",
    "                output_per_filter_row = []\n",
    "                # This projects from overflowing through columns (Verically)\n",
    "                if(j > self.input_size - self.filter_shape - 1):\n",
    "                    break\n",
    "                    \n",
    "                for k in range (self.input_size):\n",
    "\n",
    "                    # This projects from overflowing through columns (Horizontally)\n",
    "                    if(k > self.input_size - self.filter_shape - 1):\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        # Do multiplication, do slice among input channels, multiply, sum and append to output_per_filter\n",
    "                        average_pooling_section = self.inputs[i, j:self.pooling_frame_size + j, k:self.pooling_frame_size+ k]\n",
    "                        average = np.mean(average_pooling_section)\n",
    "                        output_per_filter_row.append(average)\n",
    "\n",
    "                    k+=self.stride\n",
    "                output_per_filter.append(output_per_filter_row)\n",
    "                j+=self.stride\n",
    "            \n",
    "            self.output[i] = output_per_filter\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.ones_like(self.inputs) # It isn't proper dinputs, but more of a initial filter that hepls to caluclate the dinputs\n",
    "        for i in range(self.input_channel):\n",
    "            for dvalue in dvalues:\n",
    "                for row in range(dvalue.shape[0]):\n",
    "                    for column in range(dvalue.shape[1]):\n",
    "                        # Now mutiply the block with specific elements\n",
    "                        # modifying the ones_like variable cause it's average, and every elements are involved\n",
    "                        row_specific = 0 if row == 0 else row * self.pooling_frame_size\n",
    "                        self.dinputs[i,row: row + self.pooling_frame_size, column: column + self.pooling_frame_size] = \\\n",
    "                        (1/(self.pooling_frame_size * 2))* dvalues\n",
    "                        \n",
    "       \n",
    "        # # This is also wrong, as I have to expand dvalues dimentsion to dinputs size, dvalues shape is row/2 * column/2\n",
    "        # self.dinputs = (1/(self.pooling_frame_size * 2)) * dvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa26e0c2-500a-44e8-8fa8-e0c7266a15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max_Pooling(Pooling):\n",
    "    def __init__(self,pooling_frame_size, input_channel, input_size, stride):\n",
    "        super().__init__(pooling_frame_size, input_channel, input_size,stride)\n",
    "\n",
    "    def pooling(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = []\n",
    "        # Entering 1 where ever the max value is choosen from\n",
    "        # By the way the filter name doesnot mean filter like for CNN, it's just used as normal english word\n",
    "        self.dinputs_max_filter = np.zeros_like(self.inputs)\n",
    "        \n",
    "        for i in range(self.input_channel):\n",
    "            output_per_filter = []\n",
    "            # Now I will iterate through column and through row\n",
    "            for j in range (self.input_size):\n",
    "\n",
    "                output_per_filter_row = []\n",
    "                # This projects from overflowing through columns (Verically)\n",
    "                if(j > self.input_size - self.filter_shape - 1):\n",
    "                    break\n",
    "                    \n",
    "                for k in range (self.input_size):\n",
    "\n",
    "                    # This projects from overflowing through columns (Horizontally)\n",
    "                    if(k > self.input_size - self.filter_shape - 1):\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        # Do multiplication, do slice among input channels, multiply, sum and append to output_per_filter\n",
    "                        average_pooling_section = self.inputs[i, j:self.pooling_frame_size + j, k:self.pooling_frame_size+ k]\n",
    "                        max_val = np.max(average_pooling_section)\n",
    "                        output_per_filter_row.append(max_val)\n",
    "\n",
    "                        row= -1\n",
    "                        column = -1\n",
    "                        flat_index_argmax = np.argmax(average_pooling_section)\n",
    "                        row = int(flat_index_argmax / self.pooling_frame_size)\n",
    "                        column = flat_index_argmax % self.pooling_frame_size\n",
    "\n",
    "                        # Might need to test if this really works or not\n",
    "                        self.dinputs[i,j+row, k+column] = 1\n",
    "\n",
    "                    k+=self.stride\n",
    "                output_per_filter.append(output_per_filter_row)\n",
    "                j+=self.stride\n",
    "            \n",
    "            self.output[i] = output_per_filter\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        for i in range(self.input_channel):\n",
    "            for dvalue in dvalues:\n",
    "                for row in range(dvalue.shape[0]):\n",
    "                    for column in range(dvalue.shape[1]):\n",
    "                        # Now mutiply the block with specific elements\n",
    "                        # modifying the ones_like variable cause it's average, and every elements are involved\n",
    "                        row_specific = 0 if row == 0 else row * self.pooling_frame_size\n",
    "                        self.dinputs_max_filter[i,row: row + self.pooling_frame_size, column: column + self.pooling_frame_size] = \\\n",
    "                        self.dinputs_max_filter * dvalues\n",
    "\n",
    "        \n",
    "        # # This is wrong, the shape is not right here dvalues has shape that is (row/2, column/2) in reference to self.dinputs\n",
    "        # self.dinputs = self.dinputs_max_filter * dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48ac282e-e9e2-4af2-a320-c388fac1c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "\n",
    "    def feed_forward(self,inputs):\n",
    "\n",
    "        #Also keeping in the inputs to make the backward pass\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Relu basically means take input value if it is bigger than 0, else just make it 0\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "139952fa-5886-4c4a-8d5a-db7f0afed7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "\n",
    "    def feed_forward(self,inputs):\n",
    "        normalized_val_exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        probabilities_value = normalized_val_exp / np.sum(normalized_val_exp, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities_value\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_softmax_output ,single_sample_CCELoss_dval) in \\\n",
    "        enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            # reshape softmax output of final layer say [1, 2, 3] is now [[1],[2],[3]]\n",
    "            single_softmax_output = single_softmax_output.reshape(-1,1)\n",
    "    \n",
    "            jacobian_matrix = np.diagflat(single_softmax_output) - \\\n",
    "                                np.dot(single_softmax_output, single_softmax_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_sample_CCELoss_dval)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d6c588-8867-4664-b04b-29cad6d3ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    # We are averaging all the errors in a batch\n",
    "    #output_val is predicted_value\n",
    "    def calculate(self,output_val,true_class):\n",
    "\n",
    "        loss_along_each_itter_inBatch = self.feed_forward(output_val,true_class)\n",
    "\n",
    "        averaged_err = np.mean(loss_along_each_itter_inBatch)\n",
    "\n",
    "        return averaged_err\n",
    "        \n",
    "\n",
    "class Loss_CrossCategorical(Loss):\n",
    "\n",
    "    def feed_forward(self,output_val,true_class):\n",
    "\n",
    "        number_of_samples = len(output_val)\n",
    "    \n",
    "    \n",
    "        # Clipping true_class prediction as it isn't 0 or 1\n",
    "    \n",
    "        # 1e-7 is lower limit and 1 - 1e-7 is upper limit\n",
    "        \n",
    "        clipped_val = np.clip(output_val, 1e-7, 1- 1e-7)\n",
    "        \n",
    "        # considering the output or true_class in in format [0,1,0,0,2,1,0]\n",
    "        # here the output class per each sample is corresponding index\n",
    "    \n",
    "        #considering true_class is in this format\n",
    "    \n",
    "        correct_confidences = clipped_val[range(number_of_samples), true_class]\n",
    "    \n",
    "        # Now calculating Loss\n",
    "    \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "    \n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # here dvalues means softmax output or say prediction in final layer\n",
    "    def backward(self, dvalues, true_class):\n",
    "\n",
    "        number_of_possible_classes = len(dvalues[0])\n",
    "\n",
    "        number_of_samples = len(dvalues)\n",
    "\n",
    "        # So that division by 0 or value near 0 doesn't divide anything\n",
    "        clipped_val = np.clip(dvalues, 1e-7, 1- 1e-7)\n",
    "\n",
    "        # One hot encoding true class through index assigning\n",
    "        true_class_eye_format = np.eye(number_of_possible_classes)[true_class]\n",
    "        \n",
    "        # The shape of true_class_eye_format will be (batch_size, number_of_possible_class) or (batch_size, final_layer_number_of_neurons)\n",
    "        # same as that of softmax\n",
    "        \n",
    "        self.dinputs = - true_class_eye_format / clipped_val\n",
    "\n",
    "        # Normalizing gradient\n",
    "        self.dinputs = self.dinputs / number_of_samples\n",
    "\n",
    "\n",
    "# Combined version of cross-categorical entropy loss and softmax partial derivate version is left on purpose\n",
    "# I will do that after some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baeea7af-d62e-42d1-a776-1d3d3f53d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def calculate(self,predictiction_prop, true_class):\n",
    "        # Calcualting argmax per row\n",
    "        predicted_class = np.argmax(predictiction_prop, axis=1)\n",
    "\n",
    "        accuracy = np.mean(predicted_class == true_class)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0eccc1f5-36a6-4bc2-a649-a537c72ef017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    def __init__(self,lr = 1.0):\n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        layer.weights -= self.learning_rate * layer.dweights\n",
    "        layer.biases -= self.learning_rate * layer.dbiases\n",
    "\n",
    "    def update_params_CNN_layer(self,layer):\n",
    "        layer.filter_weights -= self.learning_rate * layer.filter_weights\n",
    "        layer.filter_biases -= self.learning_rate * layer.filter_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85ec3b-c24f-4c89-b0d6-41b14565a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
