{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b36ac7b-2d5c-4569-af14-ab28ce816b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57f2fec7-3edc-435b-b765-5023cd49b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c9d571c-26fb-4cdd-985d-be243b56323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7af9b616-28f1-454f-8e60-5aca3d86fe84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_dataset[0]['image']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4fe0f21-f5ac-49f1-87e1-1392d7f50ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing LeNet\n",
    "# Initial Layer of convolution with feature map/ filter count of 6\n",
    "# Kernel size 5, output size will be (28 - 5) + 1 = 24\n",
    "# Let's build convolution Layer, so that I could generalize any architecture implementation\n",
    "class Conv_Layer:\n",
    "    def __init__(self, input_channel, input_size, stride, filter_shape, output_chanel_or_filter_num):\n",
    "        self.input_channel = input_channel # channel means to multiply with same filter in same place and adding the input_channel times\n",
    "        self.input_size = input_size # size of input or for first conv layer, it will be image resolution\n",
    "        self.stride = stride\n",
    "        self.filter_shape = filter_shape\n",
    "        self.output_chanel = output_chanel_or_filter_num # This could also be called the number of filer numbers\n",
    "        self.initialize_filer_weights()\n",
    "        self.initialize_filter_biases()\n",
    "\n",
    "        # In a normalize standarizarion here,\n",
    "        # the shape of input is (input_channel, input_shape, input_shape)\n",
    "    \n",
    "    def initialize_filer_weights(self):\n",
    "        # This structure defines every filter weights for each input channel, and it goes for each output channel\n",
    "        self.filter_weights = 0.01 * np.random.randn(self.output_chanel, self.input_channel,self.filter_shape,self.filter_shape)\n",
    "\n",
    "    def initialize_filter_biases(self):\n",
    "        self.filter_biases = np.zeros((self.output_chanel, 1))\n",
    "        # print(f'biases:: {self.filter_biases}')\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.inputs = input\n",
    "        self.output = []\n",
    "        for i, filter in enumerate(self.filter_weights):\n",
    "            output_per_filter = []\n",
    "            # Now I will iterate through column and through row\n",
    "            for row in range (0,self.input_size,self.stride):\n",
    "\n",
    "                output_per_filter_row = []\n",
    "                # This projects from overflowing through columns (Verically)\n",
    "                if(row > self.input_size - self.filter_shape):\n",
    "                    break\n",
    "                    \n",
    "                for column in range (0,self.input_size,self.stride):\n",
    "\n",
    "                    # This projects from overflowing through columns (Horizontally)\n",
    "                    if(column > self.input_size - self.filter_shape):\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        # Do multiplication, do slice among input channels, multiply, sum and append to output_per_filter\n",
    "                        multi_channel_filter_feature_block =  self.inputs[:,row:self.filter_shape + row, column:self.filter_shape + column]\n",
    "                        # Now multiplying each of the arrays with filter, and adding each and each one of them next\n",
    "                        filter_applied_frame = multi_channel_filter_feature_block * filter\n",
    "                        # added all filter per output channel fashion\n",
    "                        # print(f'i is:: {i} and {self.filter_biases[0]}, column is {column}')\n",
    "                        output_per_filter_row.append(filter_applied_frame.sum() + self.filter_biases[i])\n",
    "\n",
    "                    \n",
    "                output_per_filter.append(output_per_filter_row)\n",
    "                \n",
    "            \n",
    "            self.output.append(output_per_filter)\n",
    "        self.output = np.array(self.output) \n",
    "\n",
    "    # Need to calculate gradients for both filters and dinputs\n",
    "    # first calculating for dinputs\n",
    "    def backward(self,dvalues):\n",
    "        self.dbiases = np.sum(dvalues, axis=(1,2))\n",
    "        self.dinputs = np.zeros_like(self.inputs , dtype=np.float64)\n",
    "        # this is dweights or dfilter_weights\n",
    "        self.dweights = np.zeros_like(self.filter_weights)\n",
    "        for i, filter in enumerate(self.filter_weights):\n",
    "\n",
    "            # Now I will iterate through column and through row\n",
    "            for row in range (0,self.output[0].shape[1],self.stride):\n",
    "\n",
    "                # This projects from overflowing through columns (Verically)\n",
    "                if(row > self.input_size - self.filter_shape):\n",
    "                    break\n",
    "\n",
    "                # As the output shape is output_channel, row, column\n",
    "                for column in range (0,self.output[0].shape[1],self.stride):\n",
    "\n",
    "                    # This projects from overflowing through columns (Horizontally)\n",
    "                    if(column > self.input_size - self.filter_shape):\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        # Do multiplication, do slice among input channels, multiply, sum and append to output_per_filter\n",
    "                        self.dinputs[:, row:row + self.filter_shape, column: column + self.filter_shape] += filter * dvalues[i,row,column]\n",
    "                        self.dweights[i, :, :,:] += dvalues[i, row, column] * self.inputs[:, row: row + self.filter_shape, \\\n",
    "                        column: column + self.filter_shape]\n",
    "\n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b80706de-f015-4491-975f-799d78ac2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no learning parameters in Pooling\n",
    "class Pooling:\n",
    "    def __init__(self,pooling_frame_size, input_channel, input_size,stride=0):\n",
    "        self.pooling_frame_size = pooling_frame_size\n",
    "        self.input_channel = input_channel\n",
    "        self.input_size = input_size\n",
    "\n",
    "        if(stride != 0):\n",
    "            self.stride = stride\n",
    "        else:\n",
    "            self.stride = self.pooling_frame_size\n",
    "        \n",
    "    def pooling(self,inputs):\n",
    "        pass\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        pass\n",
    "\n",
    "class Average_Pooling(Pooling):\n",
    "    def __init__(self,pooling_frame_size, input_channel, input_size, stride=0):\n",
    "        super().__init__(pooling_frame_size, input_channel, input_size,stride)\n",
    "        \n",
    "\n",
    "    def pooling(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = []\n",
    "        \n",
    "        for i in range(self.input_channel):\n",
    "            output_per_filter = []\n",
    "            # Now I will iterate through column and through row\n",
    "            for j in range (0,self.input_size,self.stride):\n",
    "\n",
    "                output_per_filter_row = []\n",
    "                # This projects from overflowing through columns (Verically)\n",
    "                if(j > self.input_size - self.pooling_frame_size):\n",
    "                    break\n",
    "                    \n",
    "                for k in range (0,self.input_size,self.stride):\n",
    "\n",
    "                    # This projects from overflowing through columns (Horizontally)\n",
    "                    if(k > self.input_size - self.pooling_frame_size):\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        # Do multiplication, do slice among input channels, multiply, sum and append to output_per_filter\n",
    "                        average_pooling_section = self.inputs[i, j:self.pooling_frame_size + j, k:self.pooling_frame_size+ k]\n",
    "                        average = np.mean(average_pooling_section)\n",
    "                        output_per_filter_row.append(average)\n",
    "\n",
    "                    \n",
    "                output_per_filter.append(output_per_filter_row)\n",
    "                \n",
    "            \n",
    "            self.output.append(output_per_filter)\n",
    "        self.output = np.array(self.output)\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = np.ones_like(self.inputs) # It isn't proper dinputs, but more of a initial filter that hepls to caluclate the dinputs\n",
    "        for i in range(self.input_channel):\n",
    "            for dvalue in dvalues:\n",
    "                for row in range(dvalue.shape[0]):\n",
    "                    for column in range(dvalue.shape[1]):\n",
    "                        # Now mutiply the block with specific elements\n",
    "                        # modifying the ones_like variable cause it's average, and every elements are involved\n",
    "                        row_specific = 0 if row == 0 else row * self.pooling_frame_size\n",
    "                        column_specific = 0 if row == 0 else row * self.pooling_frame_size\n",
    "                        self.dinputs[i,row_specific: row_specific + self.pooling_frame_size, column_specific: column_specific \\\n",
    "                        + self.pooling_frame_size] += (1/(self.pooling_frame_size * 2))* dvalue[row,column]\n",
    "                        \n",
    "       \n",
    "        # # This would also be wrong, as I have to expand dvalues dimentsion to dinputs size, dvalues shape is row/2 * column/2\n",
    "        # self.dinputs = (1/(self.pooling_frame_size * 2)) * dvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa26e0c2-500a-44e8-8fa8-e0c7266a15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max_Pooling(Pooling):\n",
    "    def __init__(self,pooling_frame_size, input_channel, input_size, stride=0):\n",
    "        super().__init__(pooling_frame_size, input_channel, input_size,stride)\n",
    "\n",
    "    def pooling(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = []\n",
    "        # Entering 1 where ever the max value is choosen from\n",
    "        # By the way the filter name doesnot mean filter like for CNN, it's just used as normal english word\n",
    "        self.dinputs_max_filter = np.zeros_like(self.inputs)\n",
    "        \n",
    "        for i in range(self.input_channel):\n",
    "            output_per_filter = []\n",
    "            # Now I will iterate through column and through row\n",
    "            for j in range (self.input_size):\n",
    "\n",
    "                output_per_filter_row = []\n",
    "                # This projects from overflowing through columns (Verically)\n",
    "                if(j > self.input_size - self.pooling_frame_size):\n",
    "                    break\n",
    "                    \n",
    "                for k in range (self.input_size):\n",
    "\n",
    "                    # This projects from overflowing through columns (Horizontally)\n",
    "                    if(k > self.input_size - self.pooling_frame_size):\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        # Do multiplication, do slice among input channels, multiply, sum and append to output_per_filter\n",
    "                        average_pooling_section = self.inputs[i, j:self.pooling_frame_size + j, k:self.pooling_frame_size+ k]\n",
    "                        max_val = np.max(average_pooling_section)\n",
    "                        output_per_filter_row.append(max_val)\n",
    "\n",
    "                        row= -1\n",
    "                        column = -1\n",
    "                        flat_index_argmax = np.argmax(average_pooling_section)\n",
    "                        row = int(flat_index_argmax / self.pooling_frame_size)\n",
    "                        column = flat_index_argmax % self.pooling_frame_size\n",
    "\n",
    "                        # Might need to test if this really works or not\n",
    "                        self.dinputs[i,j+row, k+column] = 1\n",
    "\n",
    "                    k+=self.stride\n",
    "                output_per_filter.append(output_per_filter_row)\n",
    "                j+=self.stride\n",
    "            \n",
    "            self.output.append(output_per_filter)\n",
    "        self.output = np.array(self.output)\n",
    "\n",
    "    # I haven't fixed this max_pooling backward, will do that very soon, just feeling lazy, could just look to average pooling, where impl is correct\n",
    "    def backward(self,dvalues):\n",
    "        for i in range(self.input_channel):\n",
    "            for dvalue in dvalues:\n",
    "                for row in range(dvalue.shape[0]):\n",
    "                    for column in range(dvalue.shape[1]):\n",
    "                        # Now mutiply the block with specific elements\n",
    "                        # modifying the ones_like variable cause it's average, and every elements are involved\n",
    "                        row_specific = 0 if row == 0 else row * self.pooling_frame_size\n",
    "                        self.dinputs_max_filter[i,row: row + self.pooling_frame_size, column: column + self.pooling_frame_size] = \\\n",
    "                        self.dinputs_max_filter * dvalues\n",
    "\n",
    "        \n",
    "        # # This is wrong, the shape is not right here dvalues has shape that is (row/2, column/2) in reference to self.dinputs\n",
    "        # self.dinputs = self.dinputs_max_filter * dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "561ec472-feec-4a06-813a-8086c4c38ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initializing weights and biases\n",
    "\n",
    "        # We are initializing the values of weight in form of (n_inputs, n_neurons) just to save us time from transposing while\n",
    "        # multiplying with batches of input as batch size would be (batch_size, n_inputs), so input @ weights\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n",
    "\n",
    "        # Initially initializing at 0\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        # keeping in inputs, to calculate gradient for weights\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48ac282e-e9e2-4af2-a320-c388fac1c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Relu:\n",
    "\n",
    "    def feed_forward(self,inputs):\n",
    "\n",
    "        #Also keeping in the inputs to make the backward pass\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Relu basically means take input value if it is bigger than 0, else just make it 0\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "139952fa-5886-4c4a-8d5a-db7f0afed7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "\n",
    "    def feed_forward(self,inputs):\n",
    "        normalized_val_exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        probabilities_value = normalized_val_exp / np.sum(normalized_val_exp, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities_value\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_softmax_output ,single_sample_CCELoss_dval) in \\\n",
    "        enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            # reshape softmax output of final layer say [1, 2, 3] is now [[1],[2],[3]]\n",
    "            single_softmax_output = single_softmax_output.reshape(-1,1)\n",
    "    \n",
    "            jacobian_matrix = np.diagflat(single_softmax_output) - \\\n",
    "                                np.dot(single_softmax_output, single_softmax_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_sample_CCELoss_dval)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0d6c588-8867-4664-b04b-29cad6d3ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    # We are averaging all the errors in a batch\n",
    "    #output_val is predicted_value\n",
    "    def calculate(self,output_val,true_class):\n",
    "\n",
    "        loss_along_each_itter_inBatch = self.feed_forward(output_val,true_class)\n",
    "\n",
    "        averaged_err = np.mean(loss_along_each_itter_inBatch)\n",
    "\n",
    "        return averaged_err\n",
    "        \n",
    "\n",
    "class Loss_CrossCategorical(Loss):\n",
    "\n",
    "    def feed_forward(self,output_val,true_class):\n",
    "\n",
    "        number_of_samples = len(output_val)\n",
    "    \n",
    "    \n",
    "        # Clipping true_class prediction as it isn't 0 or 1\n",
    "    \n",
    "        # 1e-7 is lower limit and 1 - 1e-7 is upper limit\n",
    "        \n",
    "        clipped_val = np.clip(output_val, 1e-7, 1- 1e-7)\n",
    "        \n",
    "        # considering the output or true_class in in format [0,1,0,0,2,1,0]\n",
    "        # here the output class per each sample is corresponding index\n",
    "    \n",
    "        #considering true_class is in this format\n",
    "    \n",
    "        correct_confidences = clipped_val[range(number_of_samples), true_class]\n",
    "    \n",
    "        # Now calculating Loss\n",
    "    \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "    \n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # here dvalues means softmax output or say prediction in final layer\n",
    "    def backward(self, dvalues, true_class):\n",
    "\n",
    "        number_of_possible_classes = len(dvalues[0])\n",
    "\n",
    "        number_of_samples = len(dvalues)\n",
    "\n",
    "        # So that division by 0 or value near 0 doesn't divide anything\n",
    "        clipped_val = np.clip(dvalues, 1e-7, 1- 1e-7)\n",
    "\n",
    "        # One hot encoding true class through index assigning\n",
    "        true_class_eye_format = np.eye(number_of_possible_classes)[true_class]\n",
    "        \n",
    "        # The shape of true_class_eye_format will be (batch_size, number_of_possible_class) or (batch_size, final_layer_number_of_neurons)\n",
    "        # same as that of softmax\n",
    "        \n",
    "        self.dinputs = - true_class_eye_format / clipped_val\n",
    "\n",
    "        # Normalizing gradient\n",
    "        self.dinputs = self.dinputs / number_of_samples\n",
    "\n",
    "\n",
    "# Combined version of cross-categorical entropy loss and softmax partial derivate version is left on purpose\n",
    "# I will do that after some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baeea7af-d62e-42d1-a776-1d3d3f53d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def calculate(self,predictiction_prop, true_class):\n",
    "        # Calcualting argmax per row\n",
    "        predicted_class = np.argmax(predictiction_prop, axis=1)\n",
    "\n",
    "        accuracy = np.mean(predicted_class == true_class)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0eccc1f5-36a6-4bc2-a649-a537c72ef017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    def __init__(self,lr = 1.0):\n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def update_params(self,layer):\n",
    "        layer.weights -= self.learning_rate * layer.dweights\n",
    "        layer.biases -= self.learning_rate * layer.dbiases\n",
    "\n",
    "    def update_params_CNN_layer(self,layer):\n",
    "        layer.filter_weights -= self.learning_rate * layer.filter_weights\n",
    "        layer.filter_biases -= self.learning_rate * layer.filter_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cd85ec3b-c24f-4c89-b0d6-41b14565a66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3033221759942712\n",
      "2.3286910624989328\n",
      "2.6267873713683136\n",
      "2.16611226188096\n",
      "2.975969756767654\n",
      "1.578006956727644\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 77\u001b[0m\n\u001b[0;32m     73\u001b[0m activation_3\u001b[38;5;241m.\u001b[39mbackward(fully_conn_layer1\u001b[38;5;241m.\u001b[39mdinputs\u001b[38;5;241m.\u001b[39mreshape(activation_3\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m     75\u001b[0m conv_layer3\u001b[38;5;241m.\u001b[39mbackward(activation_3\u001b[38;5;241m.\u001b[39mdinputs)\n\u001b[1;32m---> 77\u001b[0m average_pooling_layer_2\u001b[38;5;241m.\u001b[39mbackward(conv_layer3\u001b[38;5;241m.\u001b[39mdinputs)\n\u001b[0;32m     79\u001b[0m activation_2\u001b[38;5;241m.\u001b[39mbackward(average_pooling_layer_2\u001b[38;5;241m.\u001b[39mdinputs)\n\u001b[0;32m     80\u001b[0m conv_layer2\u001b[38;5;241m.\u001b[39mbackward(activation_2\u001b[38;5;241m.\u001b[39mdinputs)\n",
      "Cell \u001b[1;32mIn[73], line 62\u001b[0m, in \u001b[0;36mAverage_Pooling.backward\u001b[1;34m(self, dvalues)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dvalue \u001b[38;5;129;01min\u001b[39;00m dvalues:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dvalue\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 62\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dvalue\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;66;03m# Now mutiply the block with specific elements\u001b[39;00m\n\u001b[0;32m     64\u001b[0m             \u001b[38;5;66;03m# modifying the ones_like variable cause it's average, and every elements are involved\u001b[39;00m\n\u001b[0;32m     65\u001b[0m             row_specific \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m row \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_frame_size\n\u001b[0;32m     66\u001b[0m             column_specific \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m row \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_frame_size\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Implementation of architecture almost same as LeNet\n",
    "\n",
    "conv_layer1 = Conv_Layer(input_channel=1,input_size=28,stride=1,filter_shape=5,output_chanel_or_filter_num=6)\n",
    "activation_1 = Activation_Relu()\n",
    "average_pooling_layer_1 = Average_Pooling(2,6,24)\n",
    "conv_layer2 = Conv_Layer(6,12,1,5,16)\n",
    "activation_2 = Activation_Relu()\n",
    "average_pooling_layer_2 = Average_Pooling(pooling_frame_size=2,input_channel=16,input_size=8)\n",
    "conv_layer3 = Conv_Layer(input_channel=16,input_size=4,stride=1,filter_shape=4,output_chanel_or_filter_num=120)\n",
    "activation_3 = Activation_Relu()\n",
    "fully_conn_layer1 = Layer(n_inputs=len(flatten_output), n_neurons=84)\n",
    "activation_4 = Activation_Relu()\n",
    "fully_conn_layer2 = Layer(n_inputs=84, n_neurons=10)\n",
    "softmax_activation = Softmax()\n",
    "\n",
    "for epoch,data in enumerate(train_dataset):\n",
    "\n",
    "    conv_layer1.forward(np.array(data['image']).reshape(1,28,28)) # Here we enter the input image\n",
    "    \n",
    "    \n",
    "    activation_1.feed_forward(conv_layer1.output)\n",
    "    \n",
    "    \n",
    "    average_pooling_layer_1.pooling(conv_layer1.output)\n",
    "    \n",
    "    \n",
    "    conv_layer2.forward(average_pooling_layer_1.output)\n",
    "\n",
    "    # print(f'conv_layer_2_output_shape::{conv_layer3.output.shape}')\n",
    "    \n",
    "    \n",
    "    activation_2.feed_forward(conv_layer2.output)\n",
    "    \n",
    "    \n",
    "    # print(f'conv_layer2_output_shape::{conv_layer2.output.shape}')\n",
    "    average_pooling_layer_2.pooling(activation_2.output)\n",
    "    \n",
    "    \n",
    "    conv_layer3.forward(average_pooling_layer_2.output)\n",
    "    \n",
    "    \n",
    "    activation_3.feed_forward(conv_layer3.output)\n",
    "    \n",
    "    flatten_output = conv_layer3.output.flatten()\n",
    "    \n",
    "    \n",
    "    fully_conn_layer1.feed_forward(np.array([flatten_output]))\n",
    "    \n",
    "    \n",
    "    activation_4.feed_forward(fully_conn_layer1.output)\n",
    "    \n",
    "    \n",
    "    fully_conn_layer2.feed_forward(fully_conn_layer1.output)\n",
    "    \n",
    "    \n",
    "    softmax_activation.feed_forward(fully_conn_layer2.output)\n",
    "    \n",
    "    loss_function = Loss_CrossCategorical()\n",
    "\n",
    "    if(epoch % 10 == 0):\n",
    "        print(loss_function.calculate(softmax_activation.output,[[data['label']]]))\n",
    "    \n",
    "    \n",
    "    # Now Do Backprop\n",
    "    loss_function.backward(softmax_activation.output,np.array([data['label']]))\n",
    "    softmax_activation.backward(loss_function.dinputs)\n",
    "\n",
    "    fully_conn_layer2.backward(softmax_activation.dinputs)\n",
    "\n",
    "    activation_4.backward(fully_conn_layer2.dinputs)\n",
    "    fully_conn_layer1.backward(activation_4.dinputs)\n",
    "    \n",
    "    activation_3.backward(fully_conn_layer1.dinputs.reshape(activation_3.output.shape))\n",
    "    \n",
    "    conv_layer3.backward(activation_3.dinputs)\n",
    "    \n",
    "    average_pooling_layer_2.backward(conv_layer3.dinputs)\n",
    "\n",
    "    activation_2.backward(average_pooling_layer_2.dinputs)\n",
    "    conv_layer2.backward(activation_2.dinputs)\n",
    "\n",
    "    average_pooling_layer_1.backward(conv_layer2.dinputs)\n",
    "\n",
    "    activation_1.backward(average_pooling_layer_1.dinputs)\n",
    "    conv_layer1.backward(activation_1.dinputs)\n",
    "\n",
    "    # Now updating the weights and biases\n",
    "    optimizer_general = Optimizer_SGD(lr=1.0)\n",
    "\n",
    "    optimizer_general.update_params(fully_conn_layer1)\n",
    "    optimizer_general.update_params(fully_conn_layer2)\n",
    "\n",
    "    optimizer_general.update_params_CNN_layer(conv_layer1)\n",
    "    optimizer_general.update_params_CNN_layer(conv_layer2)\n",
    "    optimizer_general.update_params_CNN_layer(conv_layer3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
